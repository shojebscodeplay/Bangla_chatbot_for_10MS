# üìò Bangla PDF Question Answering System (RAG-based QA) Assignment of 10 Minute School

A modular Retrieval-Augmented Generation (RAG) based system for answering questions from Bangla literature, specifically tested on Rabindranath Tagore's famous story **"‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ"**.
This project integrates PDF parsing, Bangla OCR preprocessing, chunking, semantic search using FAISS, and LLM-based response generation with memory. 

---
# API Documentation:
The model I am using for Bangla text is too large to deploy on Render, so I cannot provide you with the API documentation link. Instead, I am sharing a video link from my local machine where you can see that if the context is provided correctly, it gives correct answers for most of the questions.
[Watch the video demo](https://drive.google.com/file/d/1zXO13mf7LjOVyoS3qW-bUNQBbaxn0PcU/view?usp=drive_link)


## üöÄ Project Motivation
The original text was highly fragmented and contained many MCQs, making it difficult for the vector database to perform accurate similarity search.
Therefore, I used a clean PDF version of "‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ" instead. Still there was problem in text so i use online OCR and get better texts.

Due to the limitations in Bangla language support in vector embedding and OCR, it is often difficult to build reliable question-answering systems. This project is an attempt to:

- Extract Bangla text from scanned/unstructured PDFs.
- Preprocess and clean noisy or broken OCR outputs.
- Chunk the cleaned data for better semantic matching.
- Answer user queries using a multilingual embedding model and a powerful LLM (`LLaMA3-70B`).

---
## üì¶ Setup Guide
### Step 1: Clone the repository
git clone https://github.com/your_username/bangla-rag-qa.git
cd bangla-rag-qa

### Step 2: Create virtual environment & activate
python -m venv venv
source venv/bin/activate   # On Windows: venv\Scripts\activate

### Step 3: Install dependencies
pip install -r requirements.txt / poetry install

### Step 4: change the repositry of pdf in code

### Step 5: Setup environment variables
cp .env.example .env Add your GROQ_API_KEY and other settings in .env

### Step 6: Run the FastAPI server
uvicorn main:app --reload

## üß∞ Tools, Libraries, Packages Used

| Tool/Library                     | Purpose                            |
|----------------------------------|------------------------------------|
| `online ocr pdf generation`     | To make sure texts are not broken  |
| `PyMuPDF (fitz)`                | PDF Text Extraction                |
| `langchain`                     | Text chunking, embeddings, LLM handling |
| `FAISS`                         | Semantic similarity search         |
| `intfloat/multilingual-e5-base` | Multilingual text embedding        |
| `LLaMA3-70B via GROQ API`       | LLM for generating answers         |
| `dotenv`                        | Environment variable handling      |
| `FastAPI`                       | API endpoint to serve queries      |

## üõ†Ô∏è Key Features

- ‚úÖ Bangla OCR + preprocessing pipeline
- ‚úÖ Trial-and-error-based optimal chunking strategy
- ‚úÖ Vector similarity search using **FAISS**
- ‚úÖ Embedding Model: `intfloat/multilingual-e5-base`
- ‚úÖ LLM Model: `llama3-70b-8192` with memory buffer for contextual QA
- ‚úÖ Modular and extendable structure
- ‚úÖ Compatible with other Bangla texts by updating PDF

## ‚ùì Key Technical Decisions & Explanations
### üìå 1. What chunking strategy did you choose (e.g., paragraph-based, sentence-based, character limit)? Why do you think it works well for semantic retrieval?

- I am using a semantic, recursive character-based chunking strategy with overlap via RecursiveCharacterTextSplitter.
Chunk size: 600 characters
Chunk overlap: 200 characters
- In many Bangla OCR outputs, sentence boundaries are often inaccurate‚Äîespecially when full stops or punctuation are misrecognized.
- A paragraph usually carries a complete semantic meaning, which is helpful for semantic retrieval.
- Longer chunks help preserve context and provide sufficient information for LLMs to generate accurate answers.
---
### üìå 2. What embedding model did you use? Why did you choose it? How does it capture the meaning of the text?
- Model used: `intfloat/multilingual-e5-base`.
- Top multilingual model with strong Bangla support.

### üìå 3. How are you comparing the query with your stored chunks? Why did you choose this similarity method and storage setup?
- I used **Cosine Similarity via FAISS**.
- A FAISS vector index is built using embeddings generated by the `multilingual-e5-base` model.
- Cosine similarity is widely used and computationally efficient for measuring semantic closeness.
- FAISS enables real-time vector retrieval, offering low latency and scalable performance.
### üìå 4. How do you ensure that the question and the document chunks are compared meaningfully? What would happen if the query is vague or missing context?
-Both query and chunks use the same intfloat/multilingual-e5-base model, placing semantically similar content close in vector space, even across languages.
-Queries are embedded and compared to chunk embeddings; three most similar chunks are retrieved as context.
- If the query is vague:
  - The LLM may hallucinate and produce less accurate responses.
### üìå 5. Do the results seem relevant? If not, what might improve them?
- **Yes**, in most cases, the answers retrieved are relevant and derived from appropriate chunks.
- In cases where the answer was unclear or incorrect:
  - The related PDF section may have had poor OCR quality or missing punctuation.
  - The query might have been too vague or overly generic.

## üõ†Ô∏è Improvement Possibilities

By using an agentic RAG approach, I can integrate external tools for search, which will help reduce hallucinations and improve answer accuracy.

## üôã‚Äç‚ôÇÔ∏è Confession

I am not a professional coder, just a curious learner. I took help from various AI applications, but I believe that with time and good guidance, coding will become a habit and much easier for me.

